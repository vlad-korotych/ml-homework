{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3\n",
    "## Сравнение интересов аудитории телеканалов НТВ и Дождь с помощью тематического моделирования LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача:\n",
    "Сравнить интересы аудитории телеканолов НТВ и Дождь с помощью методов тематического моделирования\n",
    "1. Получить данные по аудитории из социальной сети ВК\n",
    "2. Зарегистрировать приложение, получить app_id, access_token\n",
    "3. Скачать данные по пользователям в каждой из групп (id групп ВК даны ниже, tvrain_id, ntv_id)\n",
    "4. Взять небольшую выборку из каждой совокупности телезрителей(около 1000-2000 человек, т.к. 300k-400k слишком много), с которыми работать дальше\n",
    "5. Обучить LDA модель на их подписках\n",
    "6. По группам, на которые подписаны эти люди, полуичть ключевые слова групп, на которые они подписаны\n",
    "7. Получить распределение интересов людей для каждой совокупности, сравнить на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import sys  \n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для использования VK API необходимо создать приложение в VK\n",
    "\n",
    "1. Создать приложение по адресу https://vk.com/apps?act=manage (кнопка \"создать приложение\")\n",
    "2. При создании указать название, описание (можно любые), категория  - прочее. Тип - standalone приложение\n",
    "3. В настройках получить **app_id**. App_id потребуется для получения access token\n",
    "4. Авторизовать пользователя (получить access token) можно по адресу: https://vk.com/dev/first_guide, в правилах нас интересует пункт 3 **Авторизация пользователя**\n",
    "5. После того, как ознакомитесь с авторизацией пользователя, скопируйте в адресную строку такой запрос https://oauth.vk.com/authorize?client_id=5490057&display=page&redirect_uri=https://oauth.vk.com/blank.html&scope=friends&response_type=token&v=5.52, где число **5490057** замените на число, которое получите для вашего **app_id**\n",
    "6. Нажмите Enter. Откроется окно с запросом прав. В нем отображаются название приложения, иконки прав доступа, и Ваши имя с фамилией. Нажмите «Разрешить». Вы попадете на новую страницу с предупреждением о том, что токен нельзя копировать и передавать третьим лицам. В адресной строке будет URL https://oauth.vk.com/blank.html, а после # Вы увидите дополнительные параметры — access_token, expires_in и user_id. Токен может выглядеть, например, так: 51eff86578a3bbbcb5c7043a122a69fd04dca057ac821dd7afd7c2d8e35b60172d45a26599c08034cc40a\n",
    "7. Токен — это Ваш ключ доступа. При выполнении определенных условий человек, получивший Ваш токен, может нанести существенный ущерб Вашим данным и данным других людей. Поэтому очень важно не передавать свой токен третьим лицам\n",
    "8. Поле expires_in содержит время жизни токена в секундах. 86400 секунд — это ровно сутки. Через сутки полученный токен перестанет действовать, для продолжения работы нужно будет получить новый по такому же алгоритму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your own app id and respective tokens\n",
    "\n",
    "# скопируйте сюда ваши app_id и access_token, полученные по методу, описанному выше\n",
    "app_id = '6985218'\n",
    "access_token = '3aab1b49c1a9f9bf99085e2c31a47f4e211115861d282ab4fec83afa9e8feee9dbf8a7da550b4868183d4'\n",
    "\n",
    "# id групп ВК Дождя и НТВ\n",
    "tvrain_id = 17568841\n",
    "ntv_id = 28658784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Павел Дуров\n"
     ]
    }
   ],
   "source": [
    "# проверка работы API и авторизации пользователя. Если возникает ошибка, то, возможно, access token необходимо обновить\n",
    "check_id = 1\n",
    "\n",
    "# api call and test\n",
    "def vk_get_response(method, parameters, token):\n",
    "    url = 'https://api.vk.com/method/' + method + '?' + parameters + '&access_token=' + token\n",
    "#     print url\n",
    "    return(requests.get(url).json())\n",
    "\n",
    "answer = vk_get_response(\n",
    "    'users.get', 'user_ids={0}&v=4.9&lang=ru'.format(check_id), access_token\n",
    ")['response']\n",
    "print(answer[0]['first_name'], answer[0]['last_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получение подписчиков телеканалов НТВ и Дождь в VK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим объекты, которые содержат всю информацию о подпиичиках соответствующих групп (указанных в domains) и сохраним их на диск. Получим в итоге два файла - **ntv_subs** и **tvrain_subs** в формате **.pkl** - питоновский формат хранения данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = ['ntv', 'tvrain']\n",
    "\n",
    "\n",
    "for group_domain in domains:\n",
    "    offset = 0\n",
    "    group_id = group_domain\n",
    "    fields = \"\"\"sex,bdate,city,country,home_town,lists,domain,has_mobile,\n",
    "    contacts,connections,education,universities,followers_count,occupation,last_seen,relation\"\"\"\n",
    "    first_sample = vk_get_response(\n",
    "        'groups.getMembers', 'group_id={0}&offset={1}&fields={2}&v=4.9&lang=ru'.format(\n",
    "            group_id, offset, fields\n",
    "        ), token=access_token\n",
    "    )\n",
    "    community_count = first_sample['response']['count']\n",
    "    community_members = []\n",
    "    for i in range(community_count // 1000 + 1):\n",
    "        offset = i * 1000\n",
    "        try:\n",
    "            answer = vk_get_response(\n",
    "                'groups.getMembers', 'group_id={0}&offset={1}&fields={2}&v=4.9&lang=ru'.format(\n",
    "                    group_id, offset, fields), token=access_token\n",
    "            )\n",
    "            print(\"Offset: \", offset)\n",
    "        except:\n",
    "            print(\"Offset: \", offset, \" Error\")\n",
    "        community_members += answer['response']['users']\n",
    "    save_obj(community_members, '{}_subs'.format(group_domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ntv = load_obj('ntv_subs')\n",
    "community_tvrain = load_obj('tvrain_subs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ntv_df = pd.DataFrame(community_ntv)\n",
    "community_tvrain_df = pd.DataFrame(community_tvrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что загрузилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375491\n",
      "444925\n"
     ]
    }
   ],
   "source": [
    "print(len(community_ntv_df))\n",
    "print(len(community_tvrain_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bdate</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>deactivated</th>\n",
       "      <th>domain</th>\n",
       "      <th>education_form</th>\n",
       "      <th>education_status</th>\n",
       "      <th>facebook</th>\n",
       "      <th>facebook_name</th>\n",
       "      <th>faculty</th>\n",
       "      <th>...</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relation</th>\n",
       "      <th>relation_partner</th>\n",
       "      <th>sex</th>\n",
       "      <th>skype</th>\n",
       "      <th>twitter</th>\n",
       "      <th>uid</th>\n",
       "      <th>universities</th>\n",
       "      <th>university</th>\n",
       "      <th>university_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rozenkat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>{'type': 'university', 'id': 1, 'name': 'СПбГУ'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>banned</td>\n",
       "      <td>id348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>id510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>banned</td>\n",
       "      <td>id619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.5.1987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ilyaa.orlov</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>914</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bdate  city  country deactivated       domain education_form  \\\n",
       "0       10.2   2.0      1.0         NaN     rozenkat            NaN   \n",
       "1        NaN   NaN      NaN      banned        id348            NaN   \n",
       "2        NaN   0.0      0.0         NaN        id510            NaN   \n",
       "3        NaN   NaN      NaN      banned        id619            NaN   \n",
       "4  20.5.1987   0.0      0.0         NaN  ilyaa.orlov            NaN   \n",
       "\n",
       "  education_status facebook facebook_name  faculty  ...  \\\n",
       "0              NaN      NaN           NaN      NaN  ...   \n",
       "1              NaN      NaN           NaN      NaN  ...   \n",
       "2              NaN      NaN           NaN      NaN  ...   \n",
       "3              NaN      NaN           NaN      NaN  ...   \n",
       "4              NaN      NaN           NaN      0.0  ...   \n",
       "\n",
       "                                         occupation relation  \\\n",
       "0  {'type': 'university', 'id': 1, 'name': 'СПбГУ'}      NaN   \n",
       "1                                               NaN      NaN   \n",
       "2                                               NaN      NaN   \n",
       "3                                               NaN      NaN   \n",
       "4                                               NaN      5.0   \n",
       "\n",
       "   relation_partner  sex  skype twitter  uid universities university  \\\n",
       "0               NaN    1    NaN     NaN  279          NaN        NaN   \n",
       "1               NaN    1    NaN     NaN  348          NaN        NaN   \n",
       "2               NaN    2    NaN     NaN  510          NaN        NaN   \n",
       "3               NaN    2    NaN     NaN  619          NaN        NaN   \n",
       "4               NaN    2    NaN     NaN  914           []        0.0   \n",
       "\n",
       "  university_name  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4                  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_ntv_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "интересный признак deactivated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deleted    35492\n",
       "banned     12794\n",
       "Name: deactivated, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_ntv_df['deactivated'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставляем строки только с пустым атрибутом deactivated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ntv_df = community_ntv_df.loc[community_ntv_df['deactivated'].isna()]\n",
    "community_tvrain_df = community_tvrain_df.loc[community_tvrain_df['deactivated'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327205\n",
      "375419\n"
     ]
    }
   ],
   "source": [
    "print(len(community_ntv_df))\n",
    "print(len(community_tvrain_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала получим всех уникальных подписчиков НТВ и Дождя с помощью unique. Далее с помощью numpy.random необходимо выбрать небольшой sample (например, по 1000 из каждой группы) таких людей и объединить их вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntv_uids = community_ntv_df.uid.unique().tolist()\n",
    "tvrain_uids = community_tvrain_df.uid.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получить общий список людей из двух выборок НТВ и Дождя, всего должно быть в итоге около 2000 человек\n",
    "uids = np.concatenate([np.random.choice(ntv_uids, size=1000), np.random.choice(tvrain_uids, size=1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1995"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uids = np.unique(uids)\n",
    "len(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 profiles done\n",
      "100 profiles done\n",
      "200 profiles done\n",
      "300 profiles done\n",
      "400 profiles done\n",
      "500 profiles done\n",
      "600 profiles done\n",
      "700 profiles done\n",
      "800 profiles done\n",
      "900 profiles done\n",
      "1000 profiles done\n",
      "1100 profiles done\n",
      "1200 profiles done\n",
      "1300 profiles done\n",
      "1400 profiles done\n",
      "1500 profiles done\n",
      "1600 profiles done\n",
      "1700 profiles done\n",
      "1800 profiles done\n",
      "1900 profiles done\n"
     ]
    }
   ],
   "source": [
    "# получить подписки этих пользователей\n",
    "print_counter = 0\n",
    "final_data = []\n",
    "\n",
    "for uid in uids:\n",
    "    try:\n",
    "        user_subs = vk_get_response(\n",
    "            'users.getSubscriptions', 'user_id={0}&v=4.9&lang=ru'.format(int(uid)), access_token\n",
    "        )\n",
    "        time.sleep(0.3)\n",
    "        final_data.append(user_subs)\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    if print_counter % 100 == 0:\n",
    "        print(\"{0} profiles done\".format(print_counter))\n",
    "    print_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'response': {'users': {'count': 2, 'items': [321, 129244038]},\n",
       "   'groups': {'count': 17,\n",
       "    'items': [63731512,\n",
       "     48944668,\n",
       "     1441,\n",
       "     17568841,\n",
       "     20225241,\n",
       "     23391543,\n",
       "     24098496,\n",
       "     25153764,\n",
       "     26270763,\n",
       "     29906641,\n",
       "     36456996,\n",
       "     36541026,\n",
       "     36592958,\n",
       "     41813928,\n",
       "     67888368,\n",
       "     72614618,\n",
       "     106838393]}}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 users\n",
      "Processed 200 users\n",
      "Processed 300 users\n",
      "Processed 300 users\n",
      "Processed 400 users\n",
      "Processed 500 users\n",
      "Processed 600 users\n",
      "Processed 600 users\n",
      "Processed 700 users\n",
      "Processed 800 users\n",
      "Processed 900 users\n",
      "Processed 900 users\n",
      "Processed 900 users\n",
      "Processed 1000 users\n",
      "Processed 1100 users\n",
      "Processed 1200 users\n",
      "Processed 1300 users\n",
      "Processed 1400 users\n",
      "Processed 1500 users\n",
      "Processed 1600 users\n"
     ]
    }
   ],
   "source": [
    "subs_list = []\n",
    "groups_freq_dict = {}\n",
    "top_n = 5\n",
    "\n",
    "for record, uid in zip(final_data, uids):\n",
    "    try:\n",
    "        user_subs = record\n",
    "        if not user_subs.get('response'):\n",
    "            user_subs = vk_get_response(\n",
    "                'users.getSubscriptions', 'user_id={0}&v=4.9&lang=ru'.format(int(uid)), access_token\n",
    "            )\n",
    "        subs_pd = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    'groups_count': user_subs['response']['groups'].get('count'),\n",
    "                    'groups_list': user_subs['response']['groups'].get('items'),\n",
    "                    'follows_count':user_subs['response']['users'].get('count'),\n",
    "                    'follows_list': user_subs['response']['users'].get('items'),\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for group_id in user_subs['response']['groups'].get('items')[:top_n]:\n",
    "            if groups_freq_dict.get(group_id):\n",
    "                groups_freq_dict[group_id] += 1\n",
    "            else:\n",
    "                groups_freq_dict[group_id] = 1\n",
    "\n",
    "        subs_pd['subs_count'] = subs_pd['groups_count'] + subs_pd['follows_count']\n",
    "        subs_list.append(subs_pd)\n",
    "    except:\n",
    "#         print(user_subs)\n",
    "        pass\n",
    "    if len(subs_list) % 100 == 0:\n",
    "        print(\"Processed {0} users\".format(len(subs_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самые популярные группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17568841, 284),\n",
       " (28658784, 162),\n",
       " (15755094, 60),\n",
       " (29534144, 41),\n",
       " (2158488, 36)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(key, val) for key, val in groups_freq_dict.items()], key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка постов со стен групп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 groups extracted\n",
      "200 groups extracted\n",
      "300 groups extracted\n",
      "400 groups extracted\n",
      "500 groups extracted\n",
      "600 groups extracted\n",
      "700 groups extracted\n",
      "800 groups extracted\n",
      "900 groups extracted\n",
      "1000 groups extracted\n",
      "1100 groups extracted\n",
      "1200 groups extracted\n",
      "1300 groups extracted\n",
      "1400 groups extracted\n",
      "1500 groups extracted\n",
      "1600 groups extracted\n",
      "1700 groups extracted\n",
      "1800 groups extracted\n",
      "1900 groups extracted\n",
      "2000 groups extracted\n",
      "2100 groups extracted\n",
      "2200 groups extracted\n",
      "2300 groups extracted\n",
      "2400 groups extracted\n",
      "2500 groups extracted\n",
      "2600 groups extracted\n",
      "2700 groups extracted\n",
      "2800 groups extracted\n",
      "2900 groups extracted\n",
      "3000 groups extracted\n",
      "3100 groups extracted\n",
      "3200 groups extracted\n",
      "3300 groups extracted\n",
      "Response error. Group id 41589556\n",
      "{'error': {'error_code': 15, 'error_msg': 'Access denied: this wall available only for community members', 'request_params': [{'key': 'oauth', 'value': '1'}, {'key': 'method', 'value': 'wall.get'}, {'key': 'owner_id', 'value': '-41589556'}, {'key': 'count', 'value': '100'}, {'key': 'fields', 'value': 'post_type,marked_as_ads'}, {'key': '', 'value': ''}, {'key': 'v', 'value': '4.9'}, {'key': 'lang', 'value': 'ru'}]}}\n",
      "3400 groups extracted\n",
      "3500 groups extracted\n",
      "3600 groups extracted\n",
      "3700 groups extracted\n",
      "3800 groups extracted\n",
      "3900 groups extracted\n",
      "4000 groups extracted\n",
      "4100 groups extracted\n",
      "Response error. Group id 9650068\n",
      "{'error': {'error_code': 15, 'error_msg': 'Access denied: this wall available only for community members', 'request_params': [{'key': 'oauth', 'value': '1'}, {'key': 'method', 'value': 'wall.get'}, {'key': 'owner_id', 'value': '-9650068'}, {'key': 'count', 'value': '100'}, {'key': 'fields', 'value': 'post_type,marked_as_ads'}, {'key': '', 'value': ''}, {'key': 'v', 'value': '4.9'}, {'key': 'lang', 'value': 'ru'}]}}\n",
      "4200 groups extracted\n",
      "4300 groups extracted\n",
      "4400 groups extracted\n",
      "4500 groups extracted\n",
      "Response error. Group id 77432443\n",
      "{'error': {'error_code': 15, 'error_msg': 'Access denied: this wall available only for community members', 'request_params': [{'key': 'oauth', 'value': '1'}, {'key': 'method', 'value': 'wall.get'}, {'key': 'owner_id', 'value': '-77432443'}, {'key': 'count', 'value': '100'}, {'key': 'fields', 'value': 'post_type,marked_as_ads'}, {'key': '', 'value': ''}, {'key': 'v', 'value': '4.9'}, {'key': 'lang', 'value': 'ru'}]}}\n",
      "4600 groups extracted\n",
      "4700 groups extracted\n",
      "4800 groups extracted\n",
      "4900 groups extracted\n"
     ]
    }
   ],
   "source": [
    "group_doc_dict = {}\n",
    "counter = 0\n",
    "groups_freq_dict_top5 = groups_freq_dict\n",
    "\n",
    "for group_id, freq in groups_freq_dict_top5.items():\n",
    "    counter += 1\n",
    "    try:\n",
    "        check = vk_get_response(\n",
    "            'wall.get',\n",
    "            'owner_id={0}&count=100&fields=post_type,marked_as_ads&&v=4.9&lang=ru'.format(int(group_id) * -1),\n",
    "            access_token\n",
    "        )\n",
    "        check = check['response']\n",
    "        group_doc = ''\n",
    "        if check[0] != 0:\n",
    "            for post in check[1:]:\n",
    "                if post.get('marked_as_ads') != 1:\n",
    "                    group_doc += post['text']\n",
    "        group_doc_dict[group_id] = group_doc\n",
    "    except:\n",
    "        print(\"Response error. Group id {0}\".format(group_id))\n",
    "        print(check)\n",
    "    if counter % 100 == 0:\n",
    "        print(\"{0} groups extracted\".format(counter))\n",
    "    time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить сырые данные по постам групп на диск\n",
    "save_obj(group_doc_dict, 'group_doc_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vlad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, по хорошему этот кусок надо переписать, как нас учили по NLP, но раз уж работает - пусть остается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrs_to_delete = string.punctuation + u'»' + u'«' + u'—' + u'“' + u'„' + u'•' + u'#'\n",
    "translation_table = {ord(c): None for c in chrs_to_delete if c != u'-'}\n",
    "units = MorphAnalyzer.DEFAULT_UNITS\n",
    "morph = MorphAnalyzer(result_type=None, units=units)\n",
    "PortSt = PorterStemmer()\n",
    "stopw = set(\n",
    "    [w for w in stopwords.words(['russian', 'english'])]\n",
    "    + [u'это', u'году', u'года', u'также', u'етот',\n",
    "       u'которые', u'который', u'которая', u'поэтому',\n",
    "       u'весь', u'свой', u'мочь', u'eтот', u'например',\n",
    "       u'какой-то', u'кто-то', u'самый', u'очень', u'несколько',\n",
    "       u'источник', u'стать', u'время', u'пока', u'однако',\n",
    "       u'около', u'немного', u'кроме', u'гораздо', u'каждый',\n",
    "       u'первый', u'вполне', u'из-за', u'из-под',\n",
    "       u'второй', u'нужно', u'нужный', u'просто', u'большой',\n",
    "       u'хороший', u'хотеть', u'начать', u'должный', u'новый', u'день',\n",
    "       u'метр', u'получить', u'далее', u'именно', u'апрель',\n",
    "       u'сообщать', u'разный', u'говорить', u'делать',\n",
    "       u'появиться', u'2016',\n",
    "       u'2015', u'получить', u'иметь', u'составить', u'дать', u'читать',\n",
    "       u'ничто', u'достаточно', u'использовать',\n",
    "       u'принять', u'практически',\n",
    "       u'находиться', u'месяц', u'достаточно', u'что-то', u'часто',\n",
    "       u'хотеть', u'начаться', u'делать', u'событие', u'составлять',\n",
    "       u'остаться', u'заявить', u'сделать', u'дело',\n",
    "       u'примерно', u'попасть', u'хотя', u'лишь', u'первое',\n",
    "       u'больший', u'решить', u'число', u'идти', u'давать', u'вопрос',\n",
    "       u'сегодня', u'часть', u'высокий', u'главный', u'случай', u'место',\n",
    "       u'конец', u'работать', u'работа', u'слово', u'важный', u'сказать']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_start = 'http[s]?://'\n",
    "url_end = (\n",
    "    '(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    ")\n",
    "pattern = url_start + url_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка слов постов групп - трансформация в \"хороший\" вид. Нормализация и стэмминг, удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dee1ca2f2d140a398e94caee8082971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4961), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "group_clean_doc_dict = {}\n",
    "#counter = 0\n",
    "\n",
    "for group_id, doc in tqdm(group_doc_dict.items()):\n",
    "    soup = BeautifulSoup(doc, 'html.parser')\n",
    "    body = ' '.join(\n",
    "        [tag.string.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "         for tag in soup.descendants if tag.string]\n",
    "    )\n",
    "    body = re.sub('\\[.*?\\]','', body)\n",
    "    body = re.sub(pattern,'', body)\n",
    "    if body != '':\n",
    "        body_clean = body.translate(translation_table).lower().strip()\n",
    "        words = word_tokenize(body_clean)\n",
    "        tokens = []\n",
    "        # stemming and text normalization\n",
    "        for word in words:\n",
    "            if re.match('^[a-z0-9-]+$', word) is not None:\n",
    "                tokens.append(PortSt.stem(word))\n",
    "            elif word.count('-') > 1:\n",
    "                tokens.append(word)\n",
    "            else:\n",
    "                normal_forms = morph.normal_forms(word)\n",
    "                tokens.append(normal_forms[0] if normal_forms else word)\n",
    "        # remove stopwords and leave unique words only\n",
    "        tokens = filter(\n",
    "            lambda token: token not in stopw, sorted(set(tokens))\n",
    "        )\n",
    "\n",
    "        # remove all words with more than 3 chars\n",
    "        tokens = filter(lambda token: len(token) > 3, tokens)\n",
    "    else:\n",
    "        tokens = []\n",
    "    #counter += 1\n",
    "    #if counter % 100 == 0:\n",
    "        #print(\"{0} docs processed\".format(counter))\n",
    "    group_clean_doc_dict[group_id] = tokens\n",
    "\n",
    "group_clean_doc_dict = {key: list(val) for key, val in group_clean_doc_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить обработанные данные на диск\n",
    "save_obj(group_clean_doc_dict, 'group_doc_dict_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение LDA модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import TextCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "class ListTextCorpus(TextCorpus):\n",
    "\n",
    "    def get_texts(self):\n",
    "        for doc in self.input:\n",
    "            yield doc\n",
    "                \n",
    "mycorp = ListTextCorpus(input=group_clean_doc_dict.values())\n",
    "justlda = LdaModel(\n",
    "    corpus=mycorp, num_topics=20, passes=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel performance\n",
      "0 нeoжидaннo гepoь poбuн пpuнять oкaзaлиcь пpecтyплeние мeлoдpaмa гepou oxoт нaчaльнuкoм\n",
      "1 жизнь стоить город человек вместе полный спасибо смочь ждать знать\n",
      "2 -согласный сказанохорошо ☝🏻от ☝🏻ный душисогласный -хорошо душихорошо -истинаот жизнь любить\n",
      "3 нoминация cмoтрить oпубликoвать пpoживaние пoбeждaть гpaбитель психологуpoвень динoзaвpамион типмна лaпкипроблема\n",
      "4 skool laurel gumкед 🍦😍кед 🍊кед бyтилиpoвaннoть угломер кpaн-нacoc мультитул пескоструйный\n",
      "5 масло ингредиент приготовление соль овощ растительный рецепт ложка перец нарезать\n",
      "6 безмеpный пpоизводитель нuawei мaгазин яблопользователь яблоkый благодаpность камеp таkий уделать\n",
      "7 convert firebird camino 1979oldsmobil impala hyundaizubastik oфopмлeнный резвиться лoджия комнaт\n",
      "8 чтoб ecть тoлькo мeнить кoгдa тeбить знaть былo дeнь этoт\n",
      "9 такогo нayчить подaрoк игноpиpoвать дoрогойя нaчинаeм зaгорелacь возврaтa чье-тo бeзразличие\n",
      "10 человек друг ждать писать знать группа новое фото жизнь найти\n",
      "11 який буде тілька такожа більша буть дуже можный коли рокіть\n",
      "12 230рубль 180рубль 80рубль 70рубль 150рубль длянос зовиракс карбамазепин ринонорма максидекс\n",
      "13 жить жизнь глаз понять никто любовь любить понимать сердце видеть\n",
      "14 житель территория гражданин сотрудник район регион глава служба государственный область\n",
      "15 basel peggi whomadewho hottest wynwood patric holmar kölsch matthia freedl\n",
      "16 love like music come time take hous record back feat\n",
      "17 олдскульно-ньюскульный сохра stanford интернетом-гигант kjellberg интернетом-ритейлер буве potenti намгея фэшн-показ\n",
      "18 идeaльнoгo пoдрoбнee радиомодуль дeшeвлe трeнирoвка винир прoвoдoть прeднaзнaчeн пoслeднeгo aвтoмaтичeски\n",
      "19 жизнь знать ждать любой жить найти друг новое выйти оставить\n"
     ]
    }
   ],
   "source": [
    "print('LdaModel performance')\n",
    "for i in range(20):\n",
    "    terms = justlda.get_topic_terms(i)\n",
    "    print(i, ' '.join(map(lambda x: mycorp.dictionary.get(x[0]), terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group distribution by the most relevant topic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10    0.52\n",
       "13    0.29\n",
       "14    0.11\n",
       "5     0.04\n",
       "0     0.02\n",
       "8     0.01\n",
       "16    0.01\n",
       "11    0.01\n",
       "6     0.00\n",
       "15    0.00\n",
       "3     0.00\n",
       "18    0.00\n",
       "2     0.00\n",
       "4     0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_dict = {key: 0 for key in range(20)}\n",
    "\n",
    "group_topics_dict_20 = {\n",
    "    group_id: dict(list(dummy_dict.items()) + justlda.get_document_topics(mycorp.dictionary.doc2bow(text)))\n",
    "    for group_id, text in group_clean_doc_dict.items()\n",
    "}\n",
    "check_pd_20 = pd.DataFrame.from_dict(group_topics_dict_20, orient='index')\n",
    "check_pd_20.head(10)\n",
    "print(\"Group distribution by the most relevant topic\")\n",
    "pd.Series.round(check_pd_20.idxmax(axis=1).value_counts() * 1. / len(check_pd_20), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump lda model to disk\n",
    "justlda.save('ldamodel_20_topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most typical groups for every topic\n",
      "0 нeoжидaннo гepoь poбuн пpuнять oкaзaлиcь пpecтyплeние мeлoдpaмa гepou oxoт нaчaльнuкoм\n",
      "Videochat/Видеочат/Webcam/Секс Гифки http://vk.com/club100491844\n",
      "Эротика | Частное фото | Секс Гифки | Сиськи http://vk.com/club65098753\n",
      "Перфоратор - Мужской журнал http://vk.com/club36574808\n",
      "HD Кино - Фильмы онлайн 2019 http://vk.com/club123915905\n",
      "НОВИНКИ КИНО 2019 http://vk.com/club90253744\n",
      "HD Фильмы - Игра Престолов http://vk.com/club157344510\n",
      "Audi|Ауди http://vk.com/club56513920\n",
      ":D Видео Comedy Club HD http://vk.com/club190902\n",
      "Весёлые картинки http://vk.com/club47813631\n",
      "Interior and Decor - Дизайн интерьера. Декор http://vk.com/club37684434\n",
      "\n",
      "1 жизнь стоить город человек вместе полный спасибо смочь ждать знать\n",
      "Приколы из школы http://vk.com/club158490202\n",
      "Смешные анекдоты http://vk.com/club23863253\n",
      "ЧП Красноярск http://vk.com/club93025573\n",
      "Твоя открыточка http://vk.com/club41495890\n",
      "1000 фактов http://vk.com/club154308202\n",
      "TopendBot http://vk.com/club174226458\n",
      "Красивые картинки http://vk.com/club31466113\n",
      "♥ Штучки + Рецепты ♥ http://vk.com/club29327763\n",
      "Демотиваторы http://vk.com/club193175\n",
      "SEX DOM 18+ http://vk.com/club4093705\n",
      "\n",
      "2 -согласный сказанохорошо ☝🏻от ☝🏻ный душисогласный -хорошо душихорошо -истинаот жизнь любить\n",
      "♔Это жизнь, брат♔ http://vk.com/club63452485\n",
      "Строго по-пацански http://vk.com/club40143205\n",
      "Хорошо сказано http://vk.com/club34152189\n",
      "Странный юмор http://vk.com/club29614963\n",
      "ЧП Красноярск http://vk.com/club93025573\n",
      "Аниме | Anime Heaven http://vk.com/club106761812\n",
      "TopendBot http://vk.com/club174226458\n",
      "1000 фактов http://vk.com/club154308202\n",
      "Твоя открыточка http://vk.com/club41495890\n",
      "Красивые картинки http://vk.com/club31466113\n",
      "\n",
      "3 нoминация cмoтрить oпубликoвать пpoживaние пoбeждaть гpaбитель психологуpoвень динoзaвpамион типмна лaпкипроблема\n",
      "Долбани меня калиткой http://vk.com/club77963997\n",
      "Приколы из школы http://vk.com/club158490202\n",
      "Странный юмор http://vk.com/club29614963\n",
      "Аниме | Anime Heaven http://vk.com/club106761812\n",
      "ЧП Красноярск http://vk.com/club93025573\n",
      "TopendBot http://vk.com/club174226458\n",
      "1000 фактов http://vk.com/club154308202\n",
      "Твоя открыточка http://vk.com/club41495890\n",
      "Красивые картинки http://vk.com/club31466113\n",
      "♥ Штучки + Рецепты ♥ http://vk.com/club29327763\n",
      "\n",
      "4 skool laurel gumкед 🍦😍кед 🍊кед бyтилиpoвaннoть угломер кpaн-нacoc мультитул пескоструйный\n",
      "Vans Off The Wall http://vk.com/club5240832\n",
      "Приколы из школы http://vk.com/club158490202\n",
      "ЖК «Сердце Одинцово» http://vk.com/club173377957\n",
      "ЧП Красноярск http://vk.com/club93025573\n",
      "TopendBot http://vk.com/club174226458\n",
      "1000 фактов http://vk.com/club154308202\n",
      "Твоя открыточка http://vk.com/club41495890\n",
      "Красивые картинки http://vk.com/club31466113\n",
      "♥ Штучки + Рецепты ♥ http://vk.com/club29327763\n",
      "SEX DOM 18+ http://vk.com/club4093705\n",
      "\n",
      "5 масло ингредиент приготовление соль овощ растительный рецепт ложка перец нарезать\n",
      "Хорошие рецепты http://vk.com/club80738991\n",
      "Cook Good - лучшие рецепты http://vk.com/club39009769\n",
      "Готовим детям. Рецепты мамам. Кулинария. Меню http://vk.com/club50294423\n",
      "Живи со вкусом. Лучшие рецепты. http://vk.com/club57207800\n",
      "Секреты кулинарии | Простые рецепты http://vk.com/club99344554\n",
      "Худей вкусно http://vk.com/club35113021\n",
      "Рецепты ПП: правильное питание, диетические http://vk.com/club72350250\n",
      "FitFood - Рецепты ПП http://vk.com/club77989566\n",
      "Кулинарные хитрости http://vk.com/club51068271\n",
      "Наглядная Кулинария - вкусные рецепты http://vk.com/club153002658\n",
      "\n",
      "6 безмеpный пpоизводитель нuawei мaгазин яблопользователь яблоkый благодаpность камеp таkий уделать\n",
      "9GAG http://vk.com/club32041317\n",
      "Окей Гоголь http://vk.com/club23936657\n",
      "♔Мама ама  Криминал♔ http://vk.com/club23385099\n",
      "♔Душа пацана♔ http://vk.com/club64299218\n",
      "✵DOSTOINSTVO✵ http://vk.com/club66797339\n",
      "❄ Бандитский двор ❄ http://vk.com/club123658671\n",
      "Я Поттероман | Гарри Поттер http://vk.com/club93441135\n",
      "Audi|Ауди http://vk.com/club56513920\n",
      "НЕНОРМАЛЬНО http://vk.com/club141959356\n",
      "Наш Гараж http://vk.com/club96884826\n",
      "\n",
      "7 convert firebird camino 1979oldsmobil impala hyundaizubastik oфopмлeнный резвиться лoджия комнaт\n",
      "►STREET RACING TEAM™ http://vk.com/club5436969\n",
      "ЖК «Сердце Одинцово» http://vk.com/club173377957\n",
      "ЧП Красноярск http://vk.com/club93025573\n",
      "TopendBot http://vk.com/club174226458\n",
      "1000 фактов http://vk.com/club154308202\n",
      "Твоя открыточка http://vk.com/club41495890\n",
      "Красивые картинки http://vk.com/club31466113\n",
      "♥ Штучки + Рецепты ♥ http://vk.com/club29327763\n",
      "SEX DOM 18+ http://vk.com/club4093705\n",
      "Демотиваторы http://vk.com/club193175\n",
      "\n",
      "8 чтoб ecть тoлькo мeнить кoгдa тeбить знaть былo дeнь этoт\n",
      "Фрэш — Только свежесть + БОТ фото-рамки http://vk.com/club24682007\n",
      "Книги http://vk.com/club44054326\n",
      "► Рецепты с изюминкой http://vk.com/club93681212\n",
      "Стихи http://vk.com/club49573982\n",
      "Гороскоп http://vk.com/club29038248\n",
      "Рецепты http://vk.com/club26740020\n",
      "Сериалы| ТНТ | СТС http://vk.com/club24412231\n",
      "Путешествия | \"Орел и Решка\" http://vk.com/club25313366\n",
      "Весёлые картинки http://vk.com/club47813631\n",
      "Polotno http://vk.com/club44601053\n",
      "\n",
      "9 такогo нayчить подaрoк игноpиpoвать дoрогойя нaчинаeм зaгорелacь возврaтa чье-тo бeзразличие\n",
      "Art House http://vk.com/club40931437\n",
      "Приколы из школы http://vk.com/club158490202\n",
      "Коротко обо мне http://vk.com/club94947512\n",
      "Аниме | Anime Heaven http://vk.com/club106761812\n",
      "ЧП Красноярск http://vk.com/club93025573\n",
      "TopendBot http://vk.com/club174226458\n",
      "♥ Штучки + Рецепты ♥ http://vk.com/club29327763\n",
      "Твоя открыточка http://vk.com/club41495890\n",
      "1000 фактов http://vk.com/club154308202\n",
      "SEX DOM 18+ http://vk.com/club4093705\n",
      "\n",
      "10 человек друг ждать писать знать группа новое фото жизнь найти\n",
      "Авто Продажа RU | Москва | Авторынок http://vk.com/club135400383\n",
      "Бесплатно Питер СПб http://vk.com/club42112043\n",
      "🚗 Продажа авто в МСК и МО ЗаАвто Москва http://vk.com/club147251533\n",
      "Авторынок | Ярославль Кострома Иваново http://vk.com/club53681953\n",
      "Тачка за сотку http://vk.com/club153161326\n",
      "ЧЕРНЫЙ РЫНОК ДОНЕЦК Объявления Барахолка http://vk.com/club108642049\n",
      "#ПАНЧЛАЙН http://vk.com/club147203786\n",
      "Авторынок Челябинск http://vk.com/club112028484\n",
      "Авторынок Волгоград Волжский http://vk.com/club118439851\n",
      "БАРАХОЛКА МОСКВА | ОТДАМ | ПРОДАМ | КУПЛЮ http://vk.com/club89503453\n",
      "\n",
      "11 який буде тілька такожа більша буть дуже можный коли рокіть\n",
      "Канал 1+1 http://vk.com/club6970317\n",
      "Красиві люди ІФ http://vk.com/club81001608\n",
      "Хтива дівка http://vk.com/club131544174\n",
      "Happy Paw http://vk.com/club36260860\n",
      "ТСН http://vk.com/club20035339\n",
      "VK Україна http://vk.com/club550910\n",
      "Т У М А Н http://vk.com/club27576692\n",
      "Факти ICTV http://vk.com/club28517772\n",
      "Типовий Журналіст / ТЖ http://vk.com/club4523709\n",
      "Speka Project - мастер класи, відео уроки http://vk.com/club20603407\n",
      "\n",
      "12 230рубль 180рубль 80рубль 70рубль 150рубль длянос зовиракс карбамазепин ринонорма максидекс\n",
      "Полезные Секреты! http://vk.com/club178047111\n",
      "Прически http://vk.com/club36785025\n",
      "Интеллектуальный журнал http://vk.com/club38000455\n",
      "Косплей, например http://vk.com/club170580894\n",
      "Братство Правильных Парней 👑 http://vk.com/club52170036\n",
      "ROUND-X http://vk.com/club8261308\n",
      "Идеи для жизни http://vk.com/club27901440\n",
      "★★★★★ FASHION MANIYA★★★★★ http://vk.com/club8715011\n",
      "PRAYFORDREAM http://vk.com/club70140733\n",
      "smrt bk [ smart book ] http://vk.com/club9170627\n",
      "\n",
      "13 жить жизнь глаз понять никто любовь любить понимать сердце видеть\n",
      "Габриэль Цуркану - психолог, гештальт терапевт. http://vk.com/club38773905\n",
      "НОВАЯ ЗЕМЛЯ http://vk.com/club39824723\n",
      "✟С нами Бог! http://vk.com/club175349000\n",
      "Джеймс Хиллман http://vk.com/club39864655\n",
      "Священник Нашего Времени http://vk.com/club76137996\n",
      "Инженер человеческих душ http://vk.com/club100958671\n",
      "Да любите друг друга | Православие ☨ http://vk.com/club158422918\n",
      "Лучшие притчи всех времен и народов http://vk.com/club27653829\n",
      "Социальная психология http://vk.com/club40346498\n",
      "ПРАВОСЛАВИЕ | ОКЕАН МУДРОСТИ † http://vk.com/club21513303\n",
      "\n",
      "14 житель территория гражданин сотрудник район регион глава служба государственный область\n",
      "Правительство России http://vk.com/club112292509\n",
      "Метагазета http://vk.com/club172782101\n",
      "МОЕ ПРАВО: юристы онлайн http://vk.com/club16462767\n",
      "Министерство обороны Республики Беларусь http://vk.com/club47970690\n",
      "МВД | ФСБ http://vk.com/club88124713\n",
      "Калининград.Ru - новости Калининграда http://vk.com/club23558538\n",
      "ГОРДОН http://vk.com/club91336393\n",
      "Ежедневные новости Владимирской области http://vk.com/club126028564\n",
      "Генеральная прокуратура Российской Федерации http://vk.com/club117116119\n",
      "Коммерсантъ в Башкортостане http://vk.com/club75618648\n",
      "\n",
      "15 basel peggi whomadewho hottest wynwood patric holmar kölsch matthia freedl\n",
      "[ ZERO DAY ] http://vk.com/club23155963\n",
      "На Случай Важных Переговоров http://vk.com/club2661911\n",
      "★ Самые красивые девушки Узбекистана ★ http://vk.com/club92646862\n",
      "smrt bk [ smart book ] http://vk.com/club9170627\n",
      "Красивые картинки http://vk.com/club31466113\n",
      "НАЦІОНАЛЬНИЙ КОРПУС http://vk.com/club1611359\n",
      "Манипуляция http://vk.com/club133192254\n",
      "Демотиваторы http://vk.com/club193175\n",
      "Чёткие Приколы http://vk.com/club30008385\n",
      "Good Story http://vk.com/club145596555\n",
      "\n",
      "16 love like music come time take hous record back feat\n",
      "Ed Sheeran http://vk.com/club137518957\n",
      "Armin van Buuren http://vk.com/club42743690\n",
      "INNA http://vk.com/club43691117\n",
      "NOMODELS http://vk.com/club28938939\n",
      "Dada Life http://vk.com/club43366042\n",
      "Hardwell http://vk.com/club56074769\n",
      "Лучшие свадебные и художественные фотографии http://vk.com/club71423403\n",
      "Fresh Electronic Music | EDM http://vk.com/club19492084\n",
      "Deepoff http://vk.com/club43478371\n",
      "David Guetta http://vk.com/club143105718\n",
      "\n",
      "17 олдскульно-ньюскульный сохра stanford интернетом-гигант kjellberg интернетом-ритейлер буве potenti намгея фэшн-показ\n",
      "Wornpath http://vk.com/club48006306\n",
      "житель интернета http://vk.com/club96267082\n",
      "Музыкальные новинки 2018 http://vk.com/club31424891\n",
      "Помнишь, я тебя любила? забудь нахрен! http://vk.com/club14947155\n",
      "Brighton Beach http://vk.com/club46521427\n",
      "БПАН http://vk.com/club24727626\n",
      "1000 фактов http://vk.com/club154308202\n",
      "Первозданная красота - Эротика 18+ http://vk.com/club52615786\n",
      "Красивые картинки http://vk.com/club31466113\n",
      "США -- гарант мировой безопасности http://vk.com/club16027048\n",
      "\n",
      "18 идeaльнoгo пoдрoбнee радиомодуль дeшeвлe трeнирoвка винир прoвoдoть прeднaзнaчeн пoслeднeгo aвтoмaтичeски\n",
      "Legacy♔ http://vk.com/club74085170\n",
      "Блондинки. Blonde bimbo http://vk.com/club96623047\n",
      "Мокрые киски. Намокшие красавицы http://vk.com/club178728593\n",
      "Страпончик http://vk.com/club171579463\n",
      "Рисованные девушки. http://vk.com/club137170345\n",
      "Магия чулок http://vk.com/club83952452\n",
      "videosos http://vk.com/club56263398\n",
      "4ch http://vk.com/club45745333\n",
      "Оh, boys! http://vk.com/club105846725\n",
      "Розовое вино ム http://vk.com/club94744067\n",
      "\n",
      "19 жизнь знать ждать любой жить найти друг новое выйти оставить\n",
      "Приколы из школы http://vk.com/club158490202\n",
      "Смешные анекдоты http://vk.com/club23863253\n",
      "ЧП Красноярск http://vk.com/club93025573\n",
      "Твоя открыточка http://vk.com/club41495890\n",
      "1000 фактов http://vk.com/club154308202\n",
      "TopendBot http://vk.com/club174226458\n",
      "Красивые картинки http://vk.com/club31466113\n",
      "♥ Штучки + Рецепты ♥ http://vk.com/club29327763\n",
      "Демотиваторы http://vk.com/club193175\n",
      "SEX DOM 18+ http://vk.com/club4093705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The most typical groups for every topic\")\n",
    "for i in range(20):\n",
    "    terms = justlda.get_topic_terms(i)\n",
    "    print(i, ' '.join(map(lambda x: mycorp.dictionary.get(x[0]), terms)))\n",
    "    typical_groups = check_pd_20[i].sort_values(ascending=False).index[:10]\n",
    "    for g in typical_groups:\n",
    "        group_info = vk_get_response(\n",
    "            'groups.getById', 'group_ids={0}&v=4.9&lang=ru'.format(g), access_token\n",
    "        )\n",
    "        print(group_info['response'][0]['name'] + ' ' + 'http://vk.com/club' + str(g))\n",
    "        time.sleep(0.3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все-таки надо было нормально обрабатывать тексты, после данной обработки очень сложно проинтерпретировать темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16,\n",
       "  '0.001*\"8910\" + 0.001*\"5453\" + 0.001*\"9213\" + 0.001*\"5385\" + 0.001*\"5536\" + 0.001*\"37652\" + 0.001*\"22448\" + 0.001*\"22751\" + 0.001*\"26311\" + 0.001*\"61411\"'),\n",
       " (14,\n",
       "  '0.001*\"5824\" + 0.001*\"4715\" + 0.001*\"6927\" + 0.001*\"6592\" + 0.001*\"6465\" + 0.001*\"448\" + 0.001*\"1226\" + 0.001*\"6563\" + 0.001*\"5750\" + 0.001*\"2831\"'),\n",
       " (15,\n",
       "  '0.000*\"131216\" + 0.000*\"131226\" + 0.000*\"131231\" + 0.000*\"131221\" + 0.000*\"131232\" + 0.000*\"48180\" + 0.000*\"67267\" + 0.000*\"131222\" + 0.000*\"131223\" + 0.000*\"131220\"'),\n",
       " (3,\n",
       "  '0.000*\"99530\" + 0.000*\"46005\" + 0.000*\"82648\" + 0.000*\"99532\" + 0.000*\"99531\" + 0.000*\"99520\" + 0.000*\"99533\" + 0.000*\"99522\" + 0.000*\"99537\" + 0.000*\"99526\"'),\n",
       " (4,\n",
       "  '0.000*\"192629\" + 0.000*\"484651\" + 0.000*\"484650\" + 0.000*\"484653\" + 0.000*\"484652\" + 0.000*\"227258\" + 0.000*\"227283\" + 0.000*\"227266\" + 0.000*\"207795\" + 0.000*\"227273\"')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "justlda.show_topics(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по всему, код обработки текстов и обучения LDA модели, который нам выдали, совсем не верный. По хорошему, его надо переделать так, как нас учили на курсе по NLP: https://github.com/vlad-korotych/netology-ds4-nlp/blob/master/2.%20Морфологический%20и%20синтаксический%20анализ/hw2.ipynb\n",
    "Пытаться что-нибудь показать с помощью этой модели бессмысленно. Очень плохо, когда обучающимся выдают плохой код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
